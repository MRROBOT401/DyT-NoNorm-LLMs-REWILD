# -*- coding: utf-8 -*-
"""train_sharegpt.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UbIlHJE3BV32NvHYSXliNwMhW2f7DROZ
"""

# --- Install Libraries ---
!pip install transformers datasets peft torchinfo timm -q
!pip install nvidia-ml-py3 -q

# --- Imports ---
import os
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from torchinfo import summary
from peft import LoraConfig, get_peft_model
from timm.layers import LayerNorm2d

# --- Setup Tokenizer and Base Model ---
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.config.pad_token_id = tokenizer.pad_token_id

# --- Dynamic Tanh ---
class DynamicTanh(nn.Module):
    def __init__(self, normalized_shape, channels_last, alpha_init_value=0.5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.alpha_init_value = alpha_init_value
        self.channels_last = channels_last

        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        if self.channels_last:
            x = x * self.weight + self.bias
        else:
            x = x * self.weight[:, None, None] + self.bias[:, None, None]
        return x

    def extra_repr(self):
        return f"normalized_shape={self.normalized_shape}, alpha_init_value={self.alpha_init_value}, channels_last={self.channels_last}"


def convert_ln_to_dyt(module):
    module_output = module
    if isinstance(module, nn.LayerNorm):
        module_output = DynamicTanh(module.normalized_shape, not isinstance(module, LayerNorm2d))
    for name, child in module.named_children():
        module_output.add_module(name, convert_ln_to_dyt(child))
    del module
    return module_output

model = convert_ln_to_dyt(model)

print(model)

# --- Summary ---
from torchinfo import summary

summary(model, input_size=(1, 128), dtypes=[torch.int64])

# --- PEFT Config (LoRA Injection) ---
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["c_attn","fc_in", "fc_out"],  # DistilGPT2 uses c_attn in attention, # c_attn + MLP layers
    task_type="CAUSAL_LM",
)

peft_model = get_peft_model(model, peft_config)

print("Trainable parameters after PEFT injection:")
peft_model.print_trainable_parameters()

# --- Model Summary ---
summary(peft_model, input_size=(1, 128), dtypes=[torch.int64])

import json

# --- Load the uploaded ShareGPT JSON ---
with open("/content/ShareGPT_V3_unfiltered_cleaned_split.json", 'r') as f:
    data = json.load(f)

print("Loaded", len(data), "conversations.")

print(len(data))
print(list(data[0].keys()))
for i in range(5):
    print(data[i])

for i in range(5):
    print(f"\nConversation {i+1}:")
    for turn in data[i]['conversations']:
        print(f"[{turn['from'].upper()}]: {turn['value']}")
    print("="*80)

# def preprocess(batch):
#     prompts = []
#     responses = []

#     for example in batch:
#         conversation = example.get("conversations", [])
#         user_message = ""
#         assistant_message = ""

#         for turn in conversation:
#             if turn.get("from") == "human" and not user_message:
#                 user_message = turn.get("value", "")
#             elif turn.get("from") == "gpt" and not assistant_message:
#                 assistant_message = turn.get("value", "")

#         if user_message and assistant_message:
#             prompts.append(user_message.strip())
#             responses.append(assistant_message.strip())

#     full_texts = [p + "\n\n" + r for p, r in zip(prompts, responses)]

#     tokenized_batch = tokenizer(
#         full_texts,
#         truncation=True,
#         padding="max_length",
#         max_length=512,
#     )

#     return tokenized_batch

# # --- Preprocess the loaded JSON
# tokenized_data = preprocess(data)
# print(f"Preprocessed {len(tokenized_data['input_ids'])} examples.")

def preprocess_single(example):
    conversation = example.get("conversations", [])
    user_message = ""
    assistant_message = ""

    for turn in conversation:
        if turn.get("from") == "human" and not user_message:
            user_message = turn.get("value", "")
        elif turn.get("from") == "gpt" and not assistant_message:
            assistant_message = turn.get("value", "")

    if user_message and assistant_message:
        prompt = user_message.strip()
        response = assistant_message.strip()
        full_text = prompt + "\n\n" + response

        tokenized = tokenizer(
            full_text,
            truncation=True,
            padding="max_length",
            max_length=512,
        )
        return tokenized
    else:
        return None

# Now, process the whole dataset properly
processed_data = []

for example in data:
    tokenized = preprocess_single(example)
    if tokenized:
        processed_data.append(tokenized)

# Convert to lists
input_ids = [d['input_ids'] for d in processed_data]
attention_mask = [d['attention_mask'] for d in processed_data]

print(f"Preprocessed {len(input_ids)} examples.")

# --- Train/Test Split manually
from sklearn.model_selection import train_test_split
import torch

input_ids = torch.tensor(tokenized_data['input_ids'])
attention_mask = torch.tensor(tokenized_data['attention_mask'])

train_inputs, val_inputs, train_masks, val_masks = train_test_split(
    input_ids, attention_mask, test_size=0.05, random_state=42
)

# Wrap into torch.utils.data.Dataset
class GPT2Dataset(torch.utils.data.Dataset):
    def __init__(self, input_ids, attention_mask):
        self.input_ids = input_ids
        self.attention_mask = attention_mask

    def __len__(self):
        return len(self.input_ids)

    def __getitem__(self, idx):
        return {
            "input_ids": self.input_ids[idx],
            "attention_mask": self.attention_mask[idx],
            "labels": self.input_ids[idx],  # Important: labels = inputs for LM
        }

train_dataset = GPT2Dataset(train_inputs, train_masks)
eval_dataset = GPT2Dataset(val_inputs, val_masks)

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=500,  # Evaluate every 500 steps
    save_strategy="steps",
    save_steps=1000,  # Save every 1000 steps
    save_total_limit=2,  # Keep last 2 checkpoints
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_ratio=0.1,
    # warmup_steps=500,
    lr_scheduler_type="cosine",  # Smooth LR decay
    optim="adamw_torch",
    weight_decay=0.01,
    learning_rate=5e-5, # SMALL LR for GPT-2 finetuning
    fp16=True,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # disable WandB
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

for name, param in peft_model.named_parameters():
    if param.requires_grad:
        print(name)

trainer.train()

import math

final_loss = trainer.evaluate()["eval_loss"]
final_perplexity = math.exp(final_loss)
print(f"Final Evaluation Perplexity: {final_perplexity}")

!pip install matplotlib -q
import matplotlib.pyplot as plt
import numpy as np

# --- Training Loss Plot
train_loss_steps = []
train_losses = []

for record in trainer.state.log_history:
    if 'loss' in record and 'learning_rate' in record:  # skip eval loss entries
        train_loss_steps.append(record['step'])
        train_losses.append(record['loss'])

plt.figure(figsize=(8,5))
plt.plot(train_loss_steps, train_losses, label='Training Loss', color='blue')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

# --- Validation Loss Plot
eval_loss_steps = []
eval_losses = []

for record in trainer.state.log_history:
    if 'eval_loss' in record:
        eval_loss_steps.append(record['step'])
        eval_losses.append(record['eval_loss'])

plt.figure(figsize=(8,5))
plt.plot(eval_loss_steps, eval_losses, label='Validation Loss', color='orange')
plt.xlabel('Training Steps')
plt.ylabel('Validation Loss')
plt.title('Validation Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

# --- Training and Validation Together
plt.figure(figsize=(8,5))
plt.plot(train_loss_steps, train_losses, label='Training Loss', color='blue')
plt.plot(eval_loss_steps, eval_losses, label='Validation Loss', color='orange')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training and Validation Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

# Validation Perplexity Progression
perplexities = [math.exp(l) for l in eval_losses]

plt.figure(figsize=(8,5))
plt.plot(eval_loss_steps, perplexities, label='Validation Perplexity', color='green')
plt.xlabel('Training Steps')
plt.ylabel('Perplexity')
plt.title('Validation Perplexity Progression')
plt.grid(True)
plt.legend()
plt.show()

# Assuming you know total params and trainable params:
total_params = 82060181  # Example for DistilGPT2
trainable_params = 147456  # LoRA trainable parameters

frozen_params = total_params - trainable_params

labels = ['Trainable (LoRA)', 'Frozen']
sizes = [trainable_params, frozen_params]
colors = ['#ff9999','#66b3ff']

plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Parameter Distribution After LoRA Injection')
plt.axis('equal')  # Equal aspect ratio
plt.show()

trainer.save_model("./distilgpt2_sharegpt_peft")  # or whatever your current name
tokenizer.save_pretrained("./distilgpt2_sharegpt_peft")

from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("./distilgpt2_sharegpt_peft")

# Load base model first
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Load PEFT LoRA weights
model = PeftModel.from_pretrained(base_model, "./distilgpt2_sharegpt_peft")

# Set padding tokens correctly
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

print("Model and tokenizer successfully loaded!")

def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

# Example:
prompt = "Explain how rainbows are formed."
response = generate_response(prompt, model, tokenizer)
print(response)
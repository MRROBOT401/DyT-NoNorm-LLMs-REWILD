# -*- coding: utf-8 -*-
"""train_alpaca_distillgpt2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/17JQU6oZfDgOrbXPq7hZ4XWEJ_jUfEWkX
"""

# --- Install Libraries ---
!pip install transformers datasets peft torchinfo timm -q
!pip install nvidia-ml-py3 -q

# --- Imports ---
import os
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from torchinfo import summary
from peft import LoraConfig, get_peft_model
from timm.layers import LayerNorm2d

# --- Setup Tokenizer and Base Model ---
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.config.pad_token_id = tokenizer.pad_token_id

# --- Dynamic Tanh ---
class DynamicTanh(nn.Module):
    def __init__(self, normalized_shape, channels_last, alpha_init_value=0.5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.alpha_init_value = alpha_init_value
        self.channels_last = channels_last

        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        if self.channels_last:
            x = x * self.weight + self.bias
        else:
            x = x * self.weight[:, None, None] + self.bias[:, None, None]
        return x

    def extra_repr(self):
        return f"normalized_shape={self.normalized_shape}, alpha_init_value={self.alpha_init_value}, channels_last={self.channels_last}"


def convert_ln_to_dyt(module):
    module_output = module
    if isinstance(module, nn.LayerNorm):
        module_output = DynamicTanh(module.normalized_shape, not isinstance(module, LayerNorm2d))
    for name, child in module.named_children():
        module_output.add_module(name, convert_ln_to_dyt(child))
    del module
    return module_output

model = convert_ln_to_dyt(model)

print(model)

# --- Summary ---
from torchinfo import summary

summary(model, input_size=(1, 128), dtypes=[torch.int64])

# --- PEFT Config (LoRA Injection) ---
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["c_attn","fc_in", "fc_out"],  # DistilGPT2 uses c_attn in attention, # c_attn + MLP layers
    task_type="CAUSAL_LM",
)

peft_model = get_peft_model(model, peft_config)

print("Trainable parameters after PEFT injection:")
peft_model.print_trainable_parameters()

# --- Model Summary ---
summary(peft_model, input_size=(1, 128), dtypes=[torch.int64])

# --- Load Alpaca Dataset ---
dataset = load_dataset("tatsu-lab/alpaca", split="train")

print(dataset)

print(dataset.features)

def preprocess(batch):
    sources = []
    targets = []

    for instr, inp, outp in zip(batch['instruction'], batch['input'], batch['output']):
        if inp.strip() != "":
            prompt = instr.strip() + "\n\n" + inp.strip()
        else:
            prompt = instr.strip()
        sources.append(prompt)
        targets.append(outp.strip())

    full_texts = [s + "\n\n" + t for s, t in zip(sources, targets)]

    tokenized_batch = tokenizer(
        full_texts,
        truncation=True,
        padding="max_length",
        max_length=512,
    )

    return tokenized_batch

tokenized_dataset = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing Alpaca dataset"
)

print(tokenized_dataset[0])

for i in range(5):
    print("Original instruction:")
    print(dataset[i]['instruction'])
    print("Original input:")
    print(dataset[i]['input'])
    print("Original output:")
    print(dataset[i]['output'])
    print("\nTokenized and Decoded Text:")
    print(tokenizer.decode(tokenized_dataset[i]['input_ids'], skip_special_tokens=True))
    print("=" * 80)

split_datasets = tokenized_dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = split_datasets['train']
eval_dataset = split_datasets['test']

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

# # --- Compute Metrics (Perplexity instead of Accuracy) ---
# import math
# import numpy as np

# def compute_metrics(eval_preds):
#     preds, labels = eval_preds
#     loss = np.mean((preds - labels) ** 2)  # Dummy fallback, but we will ignore

#     # Hugging Face actually provides loss separately
#     try:
#         perplexity = math.exp(loss)
#     except OverflowError:
#         perplexity = float("inf")

#     return {
#         "eval_loss": loss,
#         "eval_perplexity": perplexity,
#     }

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=500,  # Evaluate every 500 steps
    save_strategy="steps",
    save_steps=1000,  # Save every 1000 steps
    save_total_limit=2,  # Keep last 2 checkpoints
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_ratio=0.1,
    # warmup_steps=500,
    lr_scheduler_type="cosine",  # Smooth LR decay
    optim="adamw_torch",
    weight_decay=0.01,
    learning_rate=5e-5, # SMALL LR for GPT-2 finetuning
    fp16=True,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # disable WandB
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

for name, param in peft_model.named_parameters():
    if param.requires_grad:
        print(name)

trainer.train()

import math

final_loss = trainer.evaluate()["eval_loss"]
final_perplexity = math.exp(final_loss)
print(f"Final Evaluation Perplexity: {final_perplexity}")

!pip install matplotlib -q
import matplotlib.pyplot as plt
import numpy as np

# Assume your logs are inside 'trainer.state.log_history'
train_loss_steps = []
train_losses = []

for record in trainer.state.log_history:
    if 'loss' in record and 'learning_rate' in record:  # skip eval loss entries
        train_loss_steps.append(record['step'])
        train_losses.append(record['loss'])

plt.figure(figsize=(8,5))
plt.plot(train_loss_steps, train_losses, label='Training Loss', color='blue')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

eval_loss_steps = []
eval_losses = []

for record in trainer.state.log_history:
    if 'eval_loss' in record:
        eval_loss_steps.append(record['step'])
        eval_losses.append(record['eval_loss'])

plt.figure(figsize=(8,5))
plt.plot(eval_loss_steps, eval_losses, label='Validation Loss', color='orange')
plt.xlabel('Training Steps')
plt.ylabel('Validation Loss')
plt.title('Validation Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(8,5))
plt.plot(train_loss_steps, train_losses, label='Training Loss', color='blue')
plt.plot(eval_loss_steps, eval_losses, label='Validation Loss', color='orange')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training and Validation Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

import math

perplexities = [math.exp(l) for l in eval_losses]

plt.figure(figsize=(8,5))
plt.plot(eval_loss_steps, perplexities, label='Validation Perplexity', color='green')
plt.xlabel('Training Steps')
plt.ylabel('Perplexity')
plt.title('Validation Perplexity Progression')
plt.grid(True)
plt.legend()
plt.show()

# Total params and trainable params were printed earlier by peft_model.print_trainable_parameters()
total_params = 82060181  # you printed this earlier for DistilGPT2 + LoRA
trainable_params = 147456  # (with LoRA)

frozen_params = total_params - trainable_params

labels = ['Trainable (LoRA)', 'Frozen']
sizes = [trainable_params, frozen_params]
colors = ['#ff9999','#66b3ff']

plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Parameter Distribution After LoRA Injection')
plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.
plt.show()

# # Function to generate text
# def generate_response(prompt, model, tokenizer, max_new_tokens=100):
#     device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
#     model = model.to(device)
#     inputs = tokenizer(prompt, return_tensors="pt").to(device)
#     with torch.no_grad():
#         outputs = model.generate(**inputs, max_new_tokens=max_new_tokens)
#     decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
#     return decoded

# # Example
# prompt = "Explain how rainbows are formed."
# response = generate_response(prompt, peft_model, tokenizer)
# print(response)

def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,               # <--- ENABLE SAMPLING
            temperature=0.7,               # <--- Slight randomness (lower = safer)
            top_k=50,                      # <--- Top-k sampling
            top_p=0.95,                    # <--- Nucleus sampling (optional)
            num_return_sequences=1         # <--- Only 1 output
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

prompt = "Explain how rainbows are formed."

# response = generate_response(prompt, peft_model, tokenizer)
# print(response)

trainer.save_model("./distilgpt2_alpaca_peft")
tokenizer.save_pretrained("./distilgpt2_alpaca_peft")

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("./distilgpt2_alpaca_peft")

# Load base model (DistilGPT2 first)
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Load PEFT fine-tuned model weights
model = PeftModel.from_pretrained(base_model, "./distilgpt2_alpaca_peft")

# Make sure padding token is correctly set (important)
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

print("Model and tokenizer successfully loaded!")

def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

prompt = "Explain how rainbows are formed."
response = generate_response(prompt, model, tokenizer)
print(response)
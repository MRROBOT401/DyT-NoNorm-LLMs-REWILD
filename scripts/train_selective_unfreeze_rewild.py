# -*- coding: utf-8 -*-
"""train_selective_peft_rewild.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jbjIW7uRIGPSOrSvwofxm7M1-0bMRXLP
"""

# --- Install Libraries ---
!pip install transformers datasets peft torchinfo timm -q
!pip install nvidia-ml-py3 -q
!pip install ipywidgets -q

# --- Imports ---
import os
import torch
import torch.nn as nn
from transformers import (
    AutoTokenizer, AutoModelForCausalLM,
    Trainer, TrainingArguments,
    DataCollatorForLanguageModeling
)
from datasets import load_dataset
from torchinfo import summary
from peft import LoraConfig, get_peft_model
from timm.layers import LayerNorm2d

# --- Setup Tokenizer and Base Model ---
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.config.pad_token_id = tokenizer.pad_token_id
model.gradient_checkpointing_enable()

# --- Dynamic Tanh ---
class DynamicTanh(nn.Module):
    def __init__(self, normalized_shape, channels_last, alpha_init_value=0.5):
        super().__init__()
        self.normalized_shape = normalized_shape
        self.alpha_init_value = alpha_init_value
        self.channels_last = channels_last

        self.alpha = nn.Parameter(torch.ones(1) * alpha_init_value)
        self.weight = nn.Parameter(torch.ones(normalized_shape))
        self.bias = nn.Parameter(torch.zeros(normalized_shape))

    def forward(self, x):
        x = torch.tanh(self.alpha * x)
        if self.channels_last:
            x = x * self.weight + self.bias
        else:
            x = x * self.weight[:, None, None] + self.bias[:, None, None]
        return x

    def extra_repr(self):
        return f"normalized_shape={self.normalized_shape}, alpha_init_value={self.alpha_init_value}, channels_last={self.channels_last}"


def convert_ln_to_dyt(module):
    module_output = module
    if isinstance(module, nn.LayerNorm):
        module_output = DynamicTanh(module.normalized_shape, not isinstance(module, LayerNorm2d))
    for name, child in module.named_children():
        module_output.add_module(name, convert_ln_to_dyt(child))
    del module
    return module_output

model = convert_ln_to_dyt(model)

# 1. Freeze all base model parameters
for param in model.parameters():
    param.requires_grad = False

# 2. Unfreeze DyT parameters
for module in model.modules():
    if isinstance(module, DynamicTanh):
        for param in module.parameters():
            param.requires_grad = True

print(model)

for name, param in model.named_parameters():
    print(f"{name:60} requires_grad = {param.requires_grad}")

trainable = 0
frozen = 0
for name, param in model.named_parameters():
    if param.requires_grad:
        trainable += param.numel()
    else:
        frozen += param.numel()

print(f"Trainable params: {trainable:,}")
print(f"Frozen params:    {frozen:,}")
print(f"Total params:     {trainable + frozen:,}")

print("Trainable parameters:")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)

# --- Summary ---
from torchinfo import summary

summary(model, input_size=(1, 128), dtypes=[torch.int64])

for name, module in model.named_modules():
    if "fc" in name:
        print(name)

# --- PEFT Config (LoRA Injection) ---
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["c_attn", "c_fc", "c_proj"],  # DistilGPT2 uses c_attn in attention, # c_attn + MLP layers
    task_type="CAUSAL_LM",
)

peft_model = get_peft_model(model, peft_config)

for name, param in peft_model.named_parameters():
    if "lora" in name and param.requires_grad:
        print(name)

print("Trainable parameters after PEFT injection:")
peft_model.print_trainable_parameters()

# --- Model Summary ---
summary(peft_model, input_size=(1, 128), dtypes=[torch.int64])

# --- Load Alpaca Dataset ---
dataset = dataset = load_dataset("chardizard/modified-rewild", split="train")

print(dataset)

print(dataset.features)

def preprocess(batch):
    full_texts = [prompt.strip() + "\n\n" + response.strip()
                  for prompt, response in zip(batch['prompt'], batch['response'])]

    tokenized_batch = tokenizer(
        full_texts,
        truncation=True,
        padding="max_length",
        max_length=256,
    )

    # Add labels for causal LM (same as input_ids)
    tokenized_batch["labels"] = tokenized_batch["input_ids"].copy()

    return tokenized_batch

tokenized_dataset = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing modified RE-WILD"
)

print(tokenized_dataset[0])

for i in range(5):
    print(f"Original prompt:\n{dataset[i]['prompt']}")
    print(f"Original response:\n{dataset[i]['response']}")
    print("\nTokenized and Decoded Text:")
    print(tokenizer.decode(tokenized_dataset[i]['input_ids'], skip_special_tokens=True))
    print("=" * 80)

split_datasets = tokenized_dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = split_datasets['train']
eval_dataset = split_datasets['test']

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=500,  # Evaluate every 500 steps
    save_strategy="steps",
    save_steps=1000,  # Save every 1000 steps
    save_total_limit=2,  # Keep last 2 checkpoints
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    warmup_ratio=0.1,
    # warmup_steps=500,
    lr_scheduler_type="cosine",  # Smooth LR decay
    optim="adamw_torch",
    weight_decay=0.01,
    learning_rate=5e-5, # SMALL LR for GPT-2 finetuning
    fp16=True,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # disable WandB
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

torch.cuda.synchronize()
torch.cuda.empty_cache()

trainer.train()

import copy
dyt_log_history = copy.deepcopy(trainer.state.log_history)

import math

final_loss = trainer.evaluate()["eval_loss"]
final_perplexity = math.exp(final_loss)
print(f"Final Evaluation Perplexity: {final_perplexity}")

# --- Setup Tokenizer and Base Model ---
tokenizer = AutoTokenizer.from_pretrained("distilgpt2")
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model.config.pad_token_id = tokenizer.pad_token_id

print(model)

for name, param in model.named_parameters():
    print(f"{name:60} requires_grad = {param.requires_grad}")

trainable = 0
frozen = 0
for name, param in model.named_parameters():
    if param.requires_grad:
        trainable += param.numel()
    else:
        frozen += param.numel()

print(f"Trainable params: {trainable:,}")
print(f"Frozen params:    {frozen:,}")
print(f"Total params:     {trainable + frozen:,}")

print("Trainable parameters:")
for name, param in model.named_parameters():
    if param.requires_grad:
        print(name)

# --- Summary ---
from torchinfo import summary

summary(model, input_size=(1, 128), dtypes=[torch.int64])

for name, module in model.named_modules():
    if "fc" in name:
        print(name)

# --- PEFT Config (LoRA Injection) ---
peft_config = LoraConfig(
    r=8,
    lora_alpha=32,
    lora_dropout=0.05,
    bias="none",
    target_modules=["c_attn", "c_fc", "c_proj"],  # DistilGPT2 uses c_attn in attention, # c_attn + MLP layers
    task_type="CAUSAL_LM",
)

peft_model = get_peft_model(model, peft_config)

for name, param in peft_model.named_parameters():
    if "lora" in name and param.requires_grad:
        print(name)

print("Trainable parameters after PEFT injection:")
peft_model.print_trainable_parameters()

# --- Model Summary ---
summary(peft_model, input_size=(1, 128), dtypes=[torch.int64])

# --- Load the Dataset ---
dataset = dataset = load_dataset("chardizard/modified-rewild", split="train")

print(dataset)

print(dataset.features)

def preprocess(batch):
    full_texts = [prompt.strip() + "\n\n" + response.strip()
                  for prompt, response in zip(batch['prompt'], batch['response'])]

    tokenized_batch = tokenizer(
        full_texts,
        truncation=True,
        padding="max_length",
        max_length=512,
    )

    # Add labels for causal LM (same as input_ids)
    tokenized_batch["labels"] = tokenized_batch["input_ids"].copy()

    return tokenized_batch

tokenized_dataset = dataset.map(
    preprocess,
    batched=True,
    remove_columns=dataset.column_names,
    desc="Tokenizing modified RE-WILD"
)

print(tokenized_dataset[0])

for i in range(5):
    print(f"Original prompt:\n{dataset[i]['prompt']}")
    print(f"Original response:\n{dataset[i]['response']}")
    print("\nTokenized and Decoded Text:")
    print(tokenizer.decode(tokenized_dataset[i]['input_ids'], skip_special_tokens=True))
    print("=" * 80)

split_datasets = tokenized_dataset.train_test_split(test_size=0.05, seed=42)
train_dataset = split_datasets['train']
eval_dataset = split_datasets['test']

from transformers import DataCollatorForLanguageModeling

data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer,
    mlm=False
)


from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./results",
    eval_strategy="steps",
    eval_steps=500,  # Evaluate every 500 steps
    save_strategy="steps",
    save_steps=1000,  # Save every 1000 steps
    save_total_limit=2,  # Keep last 2 checkpoints
    load_best_model_at_end=True,
    metric_for_best_model="eval_loss",
    num_train_epochs=1,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=8,
    warmup_ratio=0.1,
    # warmup_steps=500,
    lr_scheduler_type="cosine",  # Smooth LR decay
    optim="adamw_torch",
    weight_decay=0.01,
    learning_rate=5e-5, # SMALL LR for GPT-2 finetuning
    fp16=True,
    logging_dir="./logs",
    logging_steps=100,
    report_to="none",  # disable WandB
)

trainer = Trainer(
    model=peft_model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
)

trainer.train()

norm_log_history = copy.deepcopy(trainer.state.log_history)

import math

final_loss = trainer.evaluate()["eval_loss"]
final_perplexity = math.exp(final_loss)
print(f"Final Evaluation Perplexity: {final_perplexity}")

import math
norm_perplexity = [math.exp(l) for l in norm_eval_losses]
dyt_perplexity = [math.exp(l) for l in dyt_eval_losses]

!pip install matplotlib -q
import matplotlib.pyplot as plt
import numpy as np

# Plot combined loss
import matplotlib.pyplot as plt

# Norm logs
norm_steps = [entry['step'] for entry in norm_log_history if 'loss' in entry and 'learning_rate' in entry]
norm_losses = [entry['loss'] for entry in norm_log_history if 'loss' in entry and 'learning_rate' in entry]

# DyT logs
dyt_steps = [entry['step'] for entry in dyt_log_history if 'loss' in entry and 'learning_rate' in entry]
dyt_losses = [entry['loss'] for entry in dyt_log_history if 'loss' in entry and 'learning_rate' in entry]

plt.figure(figsize=(8,5))
plt.plot(norm_steps, norm_losses, label="Norm Training Loss", color='blue')
plt.plot(dyt_steps, dyt_losses, label="DyT Training Loss", color='green')
plt.xlabel("Training Steps")
plt.ylabel("Loss")
plt.title("Training Loss: DyT vs Normalization")
plt.legend()
plt.grid(True)
plt.show()

# Norm eval logs
norm_eval_steps = [r['step'] for r in norm_eval_log if 'eval_loss' in r]
norm_eval_losses = [r['eval_loss'] for r in norm_eval_log if 'eval_loss' in r]

# DyT eval logs
dyt_eval_steps = [r['step'] for r in dyt_eval_log if 'eval_loss' in r]
dyt_eval_losses = [r['eval_loss'] for r in dyt_eval_log if 'eval_loss' in r]

# Plot
plt.figure(figsize=(8,5))
plt.plot(norm_eval_steps, norm_eval_losses, label='Norm Eval Loss', color='orange')
plt.plot(dyt_eval_steps, dyt_eval_losses, label='DyT Eval Loss', color='green')
plt.xlabel('Training Steps')
plt.ylabel('Validation Loss')
plt.title('Validation Loss: DyT vs Norm')
plt.grid(True)
plt.legend()
plt.show()

plt.figure(figsize=(8,5))
plt.plot(train_loss_steps, train_losses, label='Training Loss', color='blue')
plt.plot(eval_loss_steps, eval_losses, label='Validation Loss', color='orange')
plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training and Validation Loss vs Steps')
plt.grid(True)
plt.legend()
plt.show()

# Norm logs
norm_train_steps = [r['step'] for r in norm_logs if 'loss' in r and 'learning_rate' in r]
norm_train_losses = [r['loss'] for r in norm_logs if 'loss' in r and 'learning_rate' in r]
norm_eval_steps = [r['step'] for r in norm_logs if 'eval_loss' in r]
norm_eval_losses = [r['eval_loss'] for r in norm_logs if 'eval_loss' in r]

# DyT logs
dyt_train_steps = [r['step'] for r in dyt_logs if 'loss' in r and 'learning_rate' in r]
dyt_train_losses = [r['loss'] for r in dyt_logs if 'loss' in r and 'learning_rate' in r]
dyt_eval_steps = [r['step'] for r in dyt_logs if 'eval_loss' in r]
dyt_eval_losses = [r['eval_loss'] for r in dyt_logs if 'eval_loss' in r]


import matplotlib.pyplot as plt

plt.figure(figsize=(10,6))

# Norm model
plt.plot(norm_train_steps, norm_train_losses, label='Norm Training Loss', color='blue', linestyle='-')
plt.plot(norm_eval_steps, norm_eval_losses, label='Norm Validation Loss', color='blue', linestyle='--')

# DyT model
plt.plot(dyt_train_steps, dyt_train_losses, label='DyT Training Loss', color='green', linestyle='-')
plt.plot(dyt_eval_steps, dyt_eval_losses, label='DyT Validation Loss', color='green', linestyle='--')

plt.xlabel('Training Steps')
plt.ylabel('Loss')
plt.title('Training and Validation Loss: Norm vs DyT')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

norm_perplexities = [math.exp(l) for l in norm_eval_losses]
dyt_perplexities = [math.exp(l) for l in dyt_eval_losses]

plt.figure(figsize=(10,6))
plt.plot(norm_eval_steps, norm_perplexities, label='Norm Validation Perplexity', color='blue', linestyle='--')
plt.plot(dyt_eval_steps, dyt_perplexities, label='DyT Validation Perplexity', color='green', linestyle='--')
plt.xlabel('Training Steps')
plt.ylabel('Perplexity')
plt.title('Validation Perplexity: Norm vs DyT')
plt.grid(True)
plt.legend()
plt.tight_layout()
plt.show()

total_params = sum(p.numel() for p in peft_model.parameters())
trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)
frozen_params = total_params - trainable_params

labels = ['Trainable (LoRA)', 'Frozen']
sizes = [trainable_params, frozen_params]
colors = ['#ff9999','#66b3ff']

plt.figure(figsize=(6,6))
plt.pie(sizes, labels=labels, colors=colors, autopct='%1.1f%%', startangle=140)
plt.title('Parameter Distribution After LoRA Injection')
plt.axis('equal')
plt.show()

def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.eval()
    model = model.to(device)

    inputs = tokenizer(prompt, return_tensors="pt").to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1
        )

    decoded = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return decoded

prompt = "Translate to English: Je suis étudiant à NYU."

dyt_output = generate_response(prompt, dyt_model, tokenizer)
norm_output = generate_response(prompt, norm_model, tokenizer)

print("DyT Model Output:\n", dyt_output)
print("\nNorm Model Output:\n", norm_output)

# For DyT model
trainer.save_model("./distilgpt2_dyt_peft")
tokenizer.save_pretrained("./distilgpt2_dyt_peft")

# For Norm model
trainer.save_model("./distilgpt2_norm_peft")
tokenizer.save_pretrained("./distilgpt2_norm_peft")

from peft import PeftModel
from transformers import AutoTokenizer, AutoModelForCausalLM

# Load tokenizer
tokenizer = AutoTokenizer.from_pretrained("./distilgpt2_dyt_peft")

# Load base model
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")

# Load fine-tuned PEFT model (DyT)
model = PeftModel.from_pretrained(base_model, "./distilgpt2_dyt_peft")

# Set pad token
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id

print("DyT model and tokenizer loaded!")


tokenizer = AutoTokenizer.from_pretrained("./distilgpt2_norm_peft")
base_model = AutoModelForCausalLM.from_pretrained("distilgpt2")
model = PeftModel.from_pretrained(base_model, "./distilgpt2_norm_peft")
tokenizer.pad_token = tokenizer.eos_token
model.config.pad_token_id = tokenizer.pad_token_id
print("Norm model and tokenizer loaded!")

def generate_response(prompt, model, tokenizer, max_new_tokens=100):
    import torch
    from transformers import AutoTokenizer

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    tokenizer.pad_token = tokenizer.eos_token
    model.config.pad_token_id = tokenizer.pad_token_id

    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(device)

    with torch.no_grad():
        outputs = model.generate(
            **inputs,
            max_new_tokens=max_new_tokens,
            do_sample=True,
            temperature=0.7,
            top_k=50,
            top_p=0.95,
            num_return_sequences=1
        )

    return tokenizer.decode(outputs[0], skip_special_tokens=True)

# For DyT
dyt_response = generate_response("Translate to English: Je suis étudiant.", dyt_model, dyt_tokenizer)

# For Norm
norm_response = generate_response("Translate to English: Je suis étudiant.", norm_model, norm_tokenizer)

from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel

# Example loading DyT model
tokenizer_dyt = AutoTokenizer.from_pretrained("./distilgpt2_dyt_peft")
base_model_dyt = AutoModelForCausalLM.from_pretrained("distilgpt2")
dyt_model = PeftModel.from_pretrained(base_model_dyt, "./distilgpt2_dyt_peft")

# Likewise for Norm

prompt = "Explain how rainbows are formed."

# Generate response from Norm model
norm_response = generate_response(prompt, norm_model, norm_tokenizer)
print("=== Norm Model Response ===")
print(norm_response)

print("\n")

# Generate response from DyT model
dyt_response = generate_response(prompt, dyt_model, dyt_tokenizer)
print("=== DyT Model Response ===")
print(dyt_response)